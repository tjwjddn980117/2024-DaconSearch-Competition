{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - Answering with Retrieval\n",
    "\n",
    "본 대회의 과제는 중앙정부 재정 정보에 대한 **검색 기능**을 개선하고 활용도를 높이는 질의응답 알고리즘을 개발하는 것입니다. <br>이를 통해 방대한 재정 데이터를 일반 국민과 전문가 모두가 쉽게 접근하고 활용할 수 있도록 하는 것이 목표입니다. <br><br>\n",
    "베이스라인에서는 평가 데이터셋만을 활용하여 source pdf 마다 Vector DB를 구축한 뒤 langchain 라이브러리와 llama-2-ko-7b 모델을 사용하여 RAG 프로세스를 통해 추론하는 과정을 담고 있습니다. <br>( train_set을 활용한 훈련 과정은 포함하지 않으며, test_set  에 대한 추론만 진행합니다. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install accelerate\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install transformers[torch] -U\n",
    "\n",
    "!pip install datasets\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install PyMuPDF\n",
    "!pip install sentence-transformers\n",
    "# !pip install faiss-gpu\n",
    "!pip install faiss-cpu\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "import pickle\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "#LORA\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path, chunk_size=800, chunk_overlap=50):\n",
    "    \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n",
    "    # PDF 파일 열기\n",
    "    doc = fitz.open(file_path)\n",
    "    text = ''\n",
    "    # 모든 페이지의 텍스트 추출\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    # 텍스트를 chunk로 분할\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunk_temp = splitter.split_text(text)\n",
    "    # Document 객체 리스트 생성\n",
    "    chunks = [Document(page_content=t) for t in chunk_temp]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-small\"):\n",
    "    \"\"\"FAISS DB 생성\"\"\"\n",
    "    # 임베딩 모델 설정\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    # FAISS DB 생성 및 반환\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"경로 유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "\n",
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path)\n",
    "        db = create_vector_db(chunks)\n",
    "        \n",
    "        # Retriever 생성\n",
    "        retriever = db.as_retriever(search_type=\"mmr\", \n",
    "                                    search_kwargs={'k': 3, 'fetch_k': 8})\n",
    "        \n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "                'db': db,\n",
    "                'retriever': retriever\n",
    "        }\n",
    "    return pdf_databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './' # Your Base Directory\n",
    "df = pd.read_csv('./test.csv')\n",
    "pdf_databases = process_pdfs_from_dataframe(df, base_directory)\n",
    "\n",
    "# PDF 데이터베이스를 pickle 파일로 저장\n",
    "with open('pdf_databases_cpu.pickle', 'wb') as f:\n",
    "    pickle.dump(pdf_databases, f)\n",
    "\n",
    "print(\"pdf_databases가 pickle 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './' # Your Base Directory\n",
    "df = pd.read_csv('./test.csv')\n",
    "with open('pdf_databases_cpu.pickle', 'rb') as f:\n",
    "    pdf_databases = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [02:06<00:00,  8.41s/it]\n"
     ]
    }
   ],
   "source": [
    "def print_model_layers(model):\n",
    "    # 모델의 모든 레이어 이름과 구조 출력\n",
    "    for name, module in model.named_modules():\n",
    "        print(f\"{name}: {module}\")\n",
    "\n",
    "# 모델 로드\n",
    "model_id = \"beomi/llama-2-ko-7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# 레이어 이름과 구조 출력 \n",
    "#print_model_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 레이어 동결 및 학습 가능 설정\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_layers(model):\n",
    "    # 모델의 모든 레이어 출력\n",
    "    for name, module in model.named_modules():\n",
    "        print(f\"{name}: {module}\")\n",
    "\n",
    "# 모델 로드 후 레이어 출력\n",
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/llama-2-ko-7b\")\n",
    "print_model_layers(model)"
   ]
  },
  {
   "cell_type": "code",

    "    # 4비트 양자화 설정\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # 모델 ID \n",
    "    model_id = \"beomi/llama-2-ko-7b\"\n",
    "\n",
    "    # 토크나이저 로드 및 설정\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "    # 모델 로드 및 양자화 설정 적용\n",
    "    #model = AutoModelForCausalLM.from_pretrained(\n",
    "    #    model_id,\n",
    "    #    quantization_config=bnb_config,\n",
    "    #    device_map=\"auto\",\n",
    "    #    trust_remote_code=True )\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "    #    quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",

    "        model=model,\n",
    "        args=training_args,\n",
    "        peft_config = peft_config,\n",
    "        train_dataset=None,  # 여기에 학습 데이터셋을 넣으세요\n",
    "        eval_dataset=None,   # 여기에 평가 데이터셋을 넣으세요\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    finetuned_model = \"finetuned_model\"\n",
    "    # Save trained model\n",
    "    trainer.model.save_pretrained(finetuned_model)\n",
    "    \n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=trainer.model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        #task=\"LlamaForCausalLM\",\n",
    "        temperature=0.2,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=128, \n",
    "    )\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    return hf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",

     ]
    }
   ],
   "source": [
    "# LLM 파이프라인\n",
    "llm = setup_llm_SFTTrainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain 을 이용한 추론"
   ]
  },
  {
   "cell_type": "code",

   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# 배치 사이즈 설정\n",
    "batch_size = 2  # 원하는 배치 크기로 설정\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for start in tqdm(range(0, len(df), batch_size), desc=\"Answering Questions\"):\n",
    "    # 현재 배치 선택\n",
    "    batch_rows = df.iloc[start:start + batch_size]\n",
    "\n",
    "    # 배치 내의 각 행 처리\n",
    "    for _, row in batch_rows.iterrows():\n",
    "        # 소스 문자열 정규화\n",
    "        source = normalize_string(row['Source'])\n",
    "        question = row['Question']\n",
    "\n",
    "        # 정규화된 키로 데이터베이스 검색\n",
    "        normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "        retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "        # RAG 체인 구성\n",
    "        template = \"\"\"\n",
    "        다음 정보를 바탕으로 질문에 답하세요:\n",
    "        {context}\n",
    "\n",
    "        질문: {question}\n",
    "\n",
    "        답변:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "        # RAG 체인 정의\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # 답변 추론\n",
    "        print(f\"Question: {question}\")\n",
    "        full_response = rag_chain.invoke(question)\n",
    "\n",
    "        print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append({\n",
    "            \"Source\": row['Source'],\n",
    "            \"Source_path\": row['Source_path'],\n",
    "            \"Question\": question,\n",
    "            \"Answer\": full_response\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",

   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"  \n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    source = normalize_string(row['Source'])\n",
    "    question = row['Question']\n",
    "\n",
    "    # 정규화된 키로 데이터베이스 검색\n",
    "    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "    retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "    # RAG 체인 구성\n",
    "    template = \"\"\"\n",
    "    다음 정보를 바탕으로 질문에 답하세요:\n",
    "    {context}\n",
    "\n",
    "    질문: {question}\n",
    "\n",
    "    답변:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # RAG 체인 정의\n",
    "    rag_chain = ( \n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 답변 추론\n",
    "    print(f\"Question: {question}\")\n",
    "    full_response = rag_chain.invoke(question)\n",
    "\n",
    "    print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        \"Source\": row['Source'],\n",
    "        \"Source_path\": row['Source_path'],\n",
    "        \"Question\": question,\n",
    "        \"Answer\": full_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# 생성된 답변을 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(\"./baseline_submission.csv\", encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
