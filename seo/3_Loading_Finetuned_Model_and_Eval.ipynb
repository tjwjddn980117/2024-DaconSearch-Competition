{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Root of Files\n",
    "    EMBEDDING_CSV_TEST = './datas/stf_e5_base_test.csv'\n",
    "\n",
    "    SAVING_FINETUNING_MODEL_DIR = \"./finetune_models/gemma_ko_9b_ver1.01\"\n",
    "\n",
    "    PIPELINE_TEMPERATURE=0.2\n",
    "    SEQ_MAX_LENGHT=450\n",
    "\n",
    "    # About Submission\n",
    "    SAMPLE_SUBMISSION = './sample_submission.csv'\n",
    "    RESULT_SUBMISSION = './gemma_9b_ver1.01_submission.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Finetuned Model (for Quantization) and Make Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_SFTTrainer_with_finetuning():\n",
    "    # 4비트 양자화 설정\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # loading model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(CFG.SAVING_FINETUNING_MODEL_DIR, \n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 trust_remote_code=True,\n",
    "                                                 device_map=\"auto\",)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.SAVING_FINETUNING_MODEL_DIR)\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=CFG.PIPELINE_TEMPERATURE,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=CFG.SEQ_MAX_LENGHT, \n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = setup_llm_SFTTrainer_with_finetuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(CFG.EMBEDDING_CSV_TEST)\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    context = normalize_string(row['Context'])\n",
    "    question = normalize_string(row['Question'])\n",
    "\n",
    "    # RAG 체인 구성\n",
    "    template = \"\"\"다음 정보를 바탕으로 질문에 답하세요. 답변은 꼭 문장으로 하세요. 주어를 꼭 적으세요. :\n",
    "    {context}\n",
    "\n",
    "    질문: {question}\n",
    "\n",
    "    답변:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # RAG 체인 정의\n",
    "    rag_chain = ( \n",
    "        prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 답변 추론\n",
    "    # print(f\"Question: {question}\")\n",
    "    full_response = rag_chain.invoke({\"context\":context, \"question\":question})\n",
    "\n",
    "    # print(f\"Answer: {full_response}\\n\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        'Question': question,\n",
    "        'Context': context,\n",
    "        'Answer': full_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSIOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(CFG.SAMPLE_SUBMISSION)\n",
    "\n",
    "# 생성된 답변을 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(CFG.RESULT_SUBMISSION, encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Search_Baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
