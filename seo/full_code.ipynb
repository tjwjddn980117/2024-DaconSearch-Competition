{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "import pickle\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Root of Files\n",
    "    BASE_DIRECTORY = './'\n",
    "    DACON_TRAIN_CSV = './train.csv'\n",
    "    DACON_TEST_CSV = './test.csv'\n",
    "\n",
    "    EMBEDDINGS_ABOUT_TRAINING_DATASET = 'databases_multilingual-e5-base_training.pickle'\n",
    "    EMBEDDINGS_ABOUT_TEST_DATASET = 'databases_multilingual-e5-base_test.pickle'\n",
    "\n",
    "    EMBEDDING_CSV_FULL_TRAIN = './datas/stf_e5_base_full_train.csv'\n",
    "    EMBEDDING_CSV_TRAIN = './datas/stf_e5_base_train.csv'\n",
    "    EMBEDDING_CSV_VAL = './datas/stf_e5_base_val.csv'\n",
    "    EMBEDDING_CSV_TEST = './datas/stf_e5_base_test.csv'\n",
    "\n",
    "    # About Embedding\n",
    "    EMBEDDING_MODEL = \"intfloat/multilingual-e5-base\"\n",
    "    CHUNK_SIZE = 800\n",
    "    CHUNK_OVERLAP = 50\n",
    "    \n",
    "    SEARCH_TYPE = \"mmr\"\n",
    "    SEARCH_KWARGS_K = 3\n",
    "    SEARCH_KWARGS_FETCH_K = 8\n",
    "\n",
    "    # About Finetuning\n",
    "    PRETRAINING_MODEL = \"rtzr/ko-gemma-2-9b-it\"\n",
    "\n",
    "    LoRA_RANK = 16\n",
    "    LoRA_ALPHA = 32\n",
    "    LoRA_DROPOUT = 0.05\n",
    "\n",
    "    TRAINING_RESUTL_DIR = './finetune_models/training_result'\n",
    "    PER_TRAIN_BATCH_SIZE =2\n",
    "    PER_EVAL_BATCH_SIZE = 2\n",
    "    NUM_TRAIN_EPOCHS = 5\n",
    "    LOGGING_DIR='./finetune_models/training_logs'\n",
    "    LOGGING_STEPS=1000\n",
    "    SAVE_STEPS=1000\n",
    "    SAVING_FINETUNING_MODEL_DIR = \"./finetune_models/gemma_ko_9b_ver1.01\"\n",
    "\n",
    "    PIPELINE_TEMPERATURE=0.2\n",
    "    SEQ_MAX_LENGHT=450\n",
    "\n",
    "    # About Submission\n",
    "    SAMPLE_SUBMISSION = './sample_submission.csv'\n",
    "    RESULT_SUBMISSION = './gemma_9b_ver1.02_submission.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path, chunk_size=CFG.CHUNK_SIZE, chunk_overlap=CFG.CHUNK_OVERLAP):\n",
    "    \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n",
    "    # PDF 파일 열기\n",
    "    doc = fitz.open(file_path)\n",
    "    text = ''\n",
    "    # 모든 페이지의 텍스트 추출\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    # 텍스트를 chunk로 분할\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunk_temp = splitter.split_text(text)\n",
    "    # Document 객체 리스트 생성\n",
    "    chunks = [Document(page_content=t) for t in chunk_temp]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vector_db(chunks, model_path=CFG.EMBEDDING_MODEL):\n",
    "    \"\"\"FAISS DB 생성\"\"\"\n",
    "    # 임베딩 모델 설정\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    # FAISS DB 생성 및 반환\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"경로 유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "\n",
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path)\n",
    "        db = create_vector_db(chunks)\n",
    "        \n",
    "        # Retriever 생성\n",
    "        retriever = db.as_retriever(search_type=CFG.SEARCH_TYPE, \n",
    "                                    search_kwargs={'k': 3, 'fetch_k': 8})\n",
    "        \n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "                'db': db,\n",
    "                'retriever': retriever\n",
    "        }\n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING에 관한 EMBEDDING DB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1-1 2024 주요 재정통계 1권...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "Processing PDFs:   6%|▋         | 1/16 [00:07<01:46,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024 나라살림 예산개요...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  12%|█▎        | 2/16 [00:14<01:39,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 재정통계해설...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  19%|█▉        | 3/16 [00:18<01:18,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 국토교통부_전세임대(융자)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 4/16 [00:22<01:00,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 고용노동부_청년일자리창출지원...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  31%|███▏      | 5/16 [00:26<00:51,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 고용노동부_내일배움카드(일반)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  38%|███▊      | 6/16 [00:30<00:44,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_노인일자리 및 사회활동지원...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|████▍     | 7/16 [00:34<00:38,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 중소벤처기업부_창업사업화지원...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 8/16 [00:38<00:35,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_생계급여...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|█████▋    | 9/16 [00:42<00:29,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 국토교통부_소규모주택정비사업...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  62%|██████▎   | 10/16 [00:46<00:24,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 국토교통부_민간임대(융자)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  69%|██████▉   | 11/16 [00:50<00:19,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 고용노동부_조기재취업수당...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 12/16 [00:53<00:15,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024년도 성과계획서(총괄편)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  81%|████████▏ | 13/16 [02:02<01:10, 23.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」 23-3호 《조세지출 연계관리》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  88%|████████▊ | 14/16 [02:14<00:39, 19.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」 22-3호 《재정융자사업》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  94%|█████████▍| 15/16 [02:30<00:18, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 월간 나라재정 2023년 12월호...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 16/16 [03:33<00:00, 13.33s/it]\n"
     ]
    }
   ],
   "source": [
    "base_directory = CFG.BASE_DIRECTORY # Your Base Directory\n",
    "train_df = pd.read_csv(CFG.DACON_TRAIN_CSV)\n",
    "train_pdf_databases = process_pdfs_from_dataframe(train_df, base_directory)\n",
    "pickle_file_path = os.path.join(base_directory, CFG.EMBEDDINGS_ABOUT_TRAINING_DATASET)\n",
    "\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(train_pdf_databases, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST에 관한 EMBEDDING DB DB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/9 [00:00<?, ?it/s]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 중소벤처기업부_혁신창업사업화자금(융자)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 1/9 [00:04<00:35,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_부모급여(영아수당) 지원...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  22%|██▏       | 2/9 [00:08<00:29,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_노인장기요양보험 사업운영...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  33%|███▎      | 3/9 [00:12<00:23,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 산업통상자원부_에너지바우처...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|████▍     | 4/9 [00:16<00:20,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 국토교통부_행복주택출자...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|█████▌    | 5/9 [00:19<00:15,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」 22-4호 《중앙-지방 간 재정조정제도》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|██████▋   | 6/9 [00:23<00:11,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  78%|███████▊  | 7/9 [00:28<00:08,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈&포커스」 22-2호 《재정성과관리제도》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|████████▉ | 8/9 [00:31<00:03,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」(신규) 통권 제1호 《우발부채》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 9/9 [00:35<00:00,  3.93s/it]\n"
     ]
    }
   ],
   "source": [
    "base_directory = CFG.BASE_DIRECTORY # Your Base Directory\n",
    "test_df = pd.read_csv(CFG.DACON_TEST_CSV)\n",
    "test_pdf_databases = process_pdfs_from_dataframe(test_df, base_directory)\n",
    "pickle_file_path = os.path.join(base_directory, CFG.EMBEDDINGS_ABOUT_TEST_DATASET)\n",
    "\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(test_pdf_databases, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDING 정보를 저장한 CSV 파일의 생성을 위한 FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context\n",
    "\n",
    "def extract_context(df, pdf_databases, data_type='train'):\n",
    "    # 결과를 저장할 리스트 초기화\n",
    "    results = []\n",
    "\n",
    "    # 배치 사이즈 설정\n",
    "    batch_size = 1  # 원하는 배치 크기로 설정\n",
    "\n",
    "    # DataFrame의 각 행에 대해 처리\n",
    "    for start in tqdm(range(0, len(df), batch_size), desc=\"Creating Q&A including RAG info\"):\n",
    "        # 현재 배치 선택\n",
    "        batch_rows = df.iloc[start:start + batch_size]\n",
    "\n",
    "        # 배치 내의 각 행 처리\n",
    "        for _, row in batch_rows.iterrows():\n",
    "            # 소스 문자열 정규화\n",
    "            source = normalize_string(row['Source'])\n",
    "            question = normalize_string(row['Question'])\n",
    "            if data_type == 'train':\n",
    "                answer = normalize_string(row['Answer'])\n",
    "\n",
    "            # 정규화된 키로 데이터베이스 검색\n",
    "            normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "            retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "            context = format_docs(retriever.invoke(question))\n",
    "\n",
    "            if data_type == 'train':\n",
    "                results.append({\n",
    "                    \"Context\" : context,\n",
    "                    \"Question\": question,\n",
    "                    \"Answer\"  : answer,\n",
    "                })\n",
    "            else:\n",
    "                results.append({\n",
    "                    \"Context\" : context,\n",
    "                    \"Question\": question,\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING에 관한 CSV 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(CFG.DACON_TRAIN_CSV)\n",
    "with open(CFG.EMBEDDINGS_ABOUT_TRAINING_DATASET, 'rb') as f:\n",
    "    train_pdf_databases = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Q&A including RAG info: 100%|██████████| 496/496 [00:17<00:00, 28.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_context = extract_context(train_df, train_pdf_databases, data_type='train')\n",
    "\n",
    "stf_full_train_df = pd.DataFrame(train_context)\n",
    "stf_train_df = pd.DataFrame(train_context[:int(len(train_context)*0.8+0.5)])\n",
    "stf_eval_df = pd.DataFrame(train_context[int(len(train_context)*0.2+0.5):])\n",
    "\n",
    "stf_full_train_df.to_csv(CFG.EMBEDDING_CSV_FULL_TRAIN, index=False, encoding=\"UTF-8\")\n",
    "stf_train_df.to_csv(CFG.EMBEDDING_CSV_TRAIN, index=False, encoding=\"UTF-8\")\n",
    "stf_eval_df.to_csv(CFG.EMBEDDING_CSV_VAL, index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST에 관한 CSV 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(CFG.DACON_TEST_CSV)\n",
    "with open(CFG.EMBEDDINGS_ABOUT_TEST_DATASET, 'rb') as f:\n",
    "    test_pdf_databases = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Q&A including RAG info: 100%|██████████| 98/98 [00:03<00:00, 28.96it/s]\n"
     ]
    }
   ],
   "source": [
    "test_context = extract_context(test_df, test_pdf_databases, data_type='test')\n",
    "\n",
    "stf_test_df = pd.DataFrame(test_context)\n",
    "\n",
    "stf_test_df.to_csv(CFG.EMBEDDING_CSV_TEST, index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading default Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# 4비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 모델 ID \n",
    "model_id = CFG.PRETRAINING_MODEL\n",
    "\n",
    "# 토크나이저 로드 및 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.use_default_system_prompt = False\n",
    "\n",
    "# 모델 로드 및 양자화 설정 적용\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show how the model look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight True\n",
      "model.layers.0.self_attn.q_proj.weight False\n",
      "model.layers.0.self_attn.k_proj.weight False\n",
      "model.layers.0.self_attn.v_proj.weight False\n",
      "model.layers.0.self_attn.o_proj.weight False\n",
      "model.layers.0.mlp.gate_proj.weight False\n",
      "model.layers.0.mlp.up_proj.weight False\n",
      "model.layers.0.mlp.down_proj.weight False\n",
      "model.layers.0.input_layernorm.weight True\n",
      "model.layers.0.post_attention_layernorm.weight True\n",
      "model.layers.0.pre_feedforward_layernorm.weight True\n",
      "model.layers.0.post_feedforward_layernorm.weight True\n",
      "model.layers.1.self_attn.q_proj.weight False\n",
      "model.layers.1.self_attn.k_proj.weight False\n",
      "model.layers.1.self_attn.v_proj.weight False\n",
      "model.layers.1.self_attn.o_proj.weight False\n",
      "model.layers.1.mlp.gate_proj.weight False\n",
      "model.layers.1.mlp.up_proj.weight False\n",
      "model.layers.1.mlp.down_proj.weight False\n",
      "model.layers.1.input_layernorm.weight True\n",
      "model.layers.1.post_attention_layernorm.weight True\n",
      "model.layers.1.pre_feedforward_layernorm.weight True\n",
      "model.layers.1.post_feedforward_layernorm.weight True\n",
      "model.layers.2.self_attn.q_proj.weight False\n",
      "model.layers.2.self_attn.k_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.weight False\n",
      "model.layers.2.self_attn.o_proj.weight False\n",
      "model.layers.2.mlp.gate_proj.weight False\n",
      "model.layers.2.mlp.up_proj.weight False\n",
      "model.layers.2.mlp.down_proj.weight False\n",
      "model.layers.2.input_layernorm.weight True\n",
      "model.layers.2.post_attention_layernorm.weight True\n",
      "model.layers.2.pre_feedforward_layernorm.weight True\n",
      "model.layers.2.post_feedforward_layernorm.weight True\n",
      "model.layers.3.self_attn.q_proj.weight False\n",
      "model.layers.3.self_attn.k_proj.weight False\n",
      "model.layers.3.self_attn.v_proj.weight False\n",
      "model.layers.3.self_attn.o_proj.weight False\n",
      "model.layers.3.mlp.gate_proj.weight False\n",
      "model.layers.3.mlp.up_proj.weight False\n",
      "model.layers.3.mlp.down_proj.weight False\n",
      "model.layers.3.input_layernorm.weight True\n",
      "model.layers.3.post_attention_layernorm.weight True\n",
      "model.layers.3.pre_feedforward_layernorm.weight True\n",
      "model.layers.3.post_feedforward_layernorm.weight True\n",
      "model.layers.4.self_attn.q_proj.weight False\n",
      "model.layers.4.self_attn.k_proj.weight False\n",
      "model.layers.4.self_attn.v_proj.weight False\n",
      "model.layers.4.self_attn.o_proj.weight False\n",
      "model.layers.4.mlp.gate_proj.weight False\n",
      "model.layers.4.mlp.up_proj.weight False\n",
      "model.layers.4.mlp.down_proj.weight False\n",
      "model.layers.4.input_layernorm.weight True\n",
      "model.layers.4.post_attention_layernorm.weight True\n",
      "model.layers.4.pre_feedforward_layernorm.weight True\n",
      "model.layers.4.post_feedforward_layernorm.weight True\n",
      "model.layers.5.self_attn.q_proj.weight False\n",
      "model.layers.5.self_attn.k_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.weight False\n",
      "model.layers.5.self_attn.o_proj.weight False\n",
      "model.layers.5.mlp.gate_proj.weight False\n",
      "model.layers.5.mlp.up_proj.weight False\n",
      "model.layers.5.mlp.down_proj.weight False\n",
      "model.layers.5.input_layernorm.weight True\n",
      "model.layers.5.post_attention_layernorm.weight True\n",
      "model.layers.5.pre_feedforward_layernorm.weight True\n",
      "model.layers.5.post_feedforward_layernorm.weight True\n",
      "model.layers.6.self_attn.q_proj.weight False\n",
      "model.layers.6.self_attn.k_proj.weight False\n",
      "model.layers.6.self_attn.v_proj.weight False\n",
      "model.layers.6.self_attn.o_proj.weight False\n",
      "model.layers.6.mlp.gate_proj.weight False\n",
      "model.layers.6.mlp.up_proj.weight False\n",
      "model.layers.6.mlp.down_proj.weight False\n",
      "model.layers.6.input_layernorm.weight True\n",
      "model.layers.6.post_attention_layernorm.weight True\n",
      "model.layers.6.pre_feedforward_layernorm.weight True\n",
      "model.layers.6.post_feedforward_layernorm.weight True\n",
      "model.layers.7.self_attn.q_proj.weight False\n",
      "model.layers.7.self_attn.k_proj.weight False\n",
      "model.layers.7.self_attn.v_proj.weight False\n",
      "model.layers.7.self_attn.o_proj.weight False\n",
      "model.layers.7.mlp.gate_proj.weight False\n",
      "model.layers.7.mlp.up_proj.weight False\n",
      "model.layers.7.mlp.down_proj.weight False\n",
      "model.layers.7.input_layernorm.weight True\n",
      "model.layers.7.post_attention_layernorm.weight True\n",
      "model.layers.7.pre_feedforward_layernorm.weight True\n",
      "model.layers.7.post_feedforward_layernorm.weight True\n",
      "model.layers.8.self_attn.q_proj.weight False\n",
      "model.layers.8.self_attn.k_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.weight False\n",
      "model.layers.8.self_attn.o_proj.weight False\n",
      "model.layers.8.mlp.gate_proj.weight False\n",
      "model.layers.8.mlp.up_proj.weight False\n",
      "model.layers.8.mlp.down_proj.weight False\n",
      "model.layers.8.input_layernorm.weight True\n",
      "model.layers.8.post_attention_layernorm.weight True\n",
      "model.layers.8.pre_feedforward_layernorm.weight True\n",
      "model.layers.8.post_feedforward_layernorm.weight True\n",
      "model.layers.9.self_attn.q_proj.weight False\n",
      "model.layers.9.self_attn.k_proj.weight False\n",
      "model.layers.9.self_attn.v_proj.weight False\n",
      "model.layers.9.self_attn.o_proj.weight False\n",
      "model.layers.9.mlp.gate_proj.weight False\n",
      "model.layers.9.mlp.up_proj.weight False\n",
      "model.layers.9.mlp.down_proj.weight False\n",
      "model.layers.9.input_layernorm.weight True\n",
      "model.layers.9.post_attention_layernorm.weight True\n",
      "model.layers.9.pre_feedforward_layernorm.weight True\n",
      "model.layers.9.post_feedforward_layernorm.weight True\n",
      "model.layers.10.self_attn.q_proj.weight False\n",
      "model.layers.10.self_attn.k_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.weight False\n",
      "model.layers.10.self_attn.o_proj.weight False\n",
      "model.layers.10.mlp.gate_proj.weight False\n",
      "model.layers.10.mlp.up_proj.weight False\n",
      "model.layers.10.mlp.down_proj.weight False\n",
      "model.layers.10.input_layernorm.weight True\n",
      "model.layers.10.post_attention_layernorm.weight True\n",
      "model.layers.10.pre_feedforward_layernorm.weight True\n",
      "model.layers.10.post_feedforward_layernorm.weight True\n",
      "model.layers.11.self_attn.q_proj.weight False\n",
      "model.layers.11.self_attn.k_proj.weight False\n",
      "model.layers.11.self_attn.v_proj.weight False\n",
      "model.layers.11.self_attn.o_proj.weight False\n",
      "model.layers.11.mlp.gate_proj.weight False\n",
      "model.layers.11.mlp.up_proj.weight False\n",
      "model.layers.11.mlp.down_proj.weight False\n",
      "model.layers.11.input_layernorm.weight True\n",
      "model.layers.11.post_attention_layernorm.weight True\n",
      "model.layers.11.pre_feedforward_layernorm.weight True\n",
      "model.layers.11.post_feedforward_layernorm.weight True\n",
      "model.layers.12.self_attn.q_proj.weight False\n",
      "model.layers.12.self_attn.k_proj.weight False\n",
      "model.layers.12.self_attn.v_proj.weight False\n",
      "model.layers.12.self_attn.o_proj.weight False\n",
      "model.layers.12.mlp.gate_proj.weight False\n",
      "model.layers.12.mlp.up_proj.weight False\n",
      "model.layers.12.mlp.down_proj.weight False\n",
      "model.layers.12.input_layernorm.weight True\n",
      "model.layers.12.post_attention_layernorm.weight True\n",
      "model.layers.12.pre_feedforward_layernorm.weight True\n",
      "model.layers.12.post_feedforward_layernorm.weight True\n",
      "model.layers.13.self_attn.q_proj.weight False\n",
      "model.layers.13.self_attn.k_proj.weight False\n",
      "model.layers.13.self_attn.v_proj.weight False\n",
      "model.layers.13.self_attn.o_proj.weight False\n",
      "model.layers.13.mlp.gate_proj.weight False\n",
      "model.layers.13.mlp.up_proj.weight False\n",
      "model.layers.13.mlp.down_proj.weight False\n",
      "model.layers.13.input_layernorm.weight True\n",
      "model.layers.13.post_attention_layernorm.weight True\n",
      "model.layers.13.pre_feedforward_layernorm.weight True\n",
      "model.layers.13.post_feedforward_layernorm.weight True\n",
      "model.layers.14.self_attn.q_proj.weight False\n",
      "model.layers.14.self_attn.k_proj.weight False\n",
      "model.layers.14.self_attn.v_proj.weight False\n",
      "model.layers.14.self_attn.o_proj.weight False\n",
      "model.layers.14.mlp.gate_proj.weight False\n",
      "model.layers.14.mlp.up_proj.weight False\n",
      "model.layers.14.mlp.down_proj.weight False\n",
      "model.layers.14.input_layernorm.weight True\n",
      "model.layers.14.post_attention_layernorm.weight True\n",
      "model.layers.14.pre_feedforward_layernorm.weight True\n",
      "model.layers.14.post_feedforward_layernorm.weight True\n",
      "model.layers.15.self_attn.q_proj.weight False\n",
      "model.layers.15.self_attn.k_proj.weight False\n",
      "model.layers.15.self_attn.v_proj.weight False\n",
      "model.layers.15.self_attn.o_proj.weight False\n",
      "model.layers.15.mlp.gate_proj.weight False\n",
      "model.layers.15.mlp.up_proj.weight False\n",
      "model.layers.15.mlp.down_proj.weight False\n",
      "model.layers.15.input_layernorm.weight True\n",
      "model.layers.15.post_attention_layernorm.weight True\n",
      "model.layers.15.pre_feedforward_layernorm.weight True\n",
      "model.layers.15.post_feedforward_layernorm.weight True\n",
      "model.layers.16.self_attn.q_proj.weight False\n",
      "model.layers.16.self_attn.k_proj.weight False\n",
      "model.layers.16.self_attn.v_proj.weight False\n",
      "model.layers.16.self_attn.o_proj.weight False\n",
      "model.layers.16.mlp.gate_proj.weight False\n",
      "model.layers.16.mlp.up_proj.weight False\n",
      "model.layers.16.mlp.down_proj.weight False\n",
      "model.layers.16.input_layernorm.weight True\n",
      "model.layers.16.post_attention_layernorm.weight True\n",
      "model.layers.16.pre_feedforward_layernorm.weight True\n",
      "model.layers.16.post_feedforward_layernorm.weight True\n",
      "model.layers.17.self_attn.q_proj.weight False\n",
      "model.layers.17.self_attn.k_proj.weight False\n",
      "model.layers.17.self_attn.v_proj.weight False\n",
      "model.layers.17.self_attn.o_proj.weight False\n",
      "model.layers.17.mlp.gate_proj.weight False\n",
      "model.layers.17.mlp.up_proj.weight False\n",
      "model.layers.17.mlp.down_proj.weight False\n",
      "model.layers.17.input_layernorm.weight True\n",
      "model.layers.17.post_attention_layernorm.weight True\n",
      "model.layers.17.pre_feedforward_layernorm.weight True\n",
      "model.layers.17.post_feedforward_layernorm.weight True\n",
      "model.layers.18.self_attn.q_proj.weight False\n",
      "model.layers.18.self_attn.k_proj.weight False\n",
      "model.layers.18.self_attn.v_proj.weight False\n",
      "model.layers.18.self_attn.o_proj.weight False\n",
      "model.layers.18.mlp.gate_proj.weight False\n",
      "model.layers.18.mlp.up_proj.weight False\n",
      "model.layers.18.mlp.down_proj.weight False\n",
      "model.layers.18.input_layernorm.weight True\n",
      "model.layers.18.post_attention_layernorm.weight True\n",
      "model.layers.18.pre_feedforward_layernorm.weight True\n",
      "model.layers.18.post_feedforward_layernorm.weight True\n",
      "model.layers.19.self_attn.q_proj.weight False\n",
      "model.layers.19.self_attn.k_proj.weight False\n",
      "model.layers.19.self_attn.v_proj.weight False\n",
      "model.layers.19.self_attn.o_proj.weight False\n",
      "model.layers.19.mlp.gate_proj.weight False\n",
      "model.layers.19.mlp.up_proj.weight False\n",
      "model.layers.19.mlp.down_proj.weight False\n",
      "model.layers.19.input_layernorm.weight True\n",
      "model.layers.19.post_attention_layernorm.weight True\n",
      "model.layers.19.pre_feedforward_layernorm.weight True\n",
      "model.layers.19.post_feedforward_layernorm.weight True\n",
      "model.layers.20.self_attn.q_proj.weight False\n",
      "model.layers.20.self_attn.k_proj.weight False\n",
      "model.layers.20.self_attn.v_proj.weight False\n",
      "model.layers.20.self_attn.o_proj.weight False\n",
      "model.layers.20.mlp.gate_proj.weight False\n",
      "model.layers.20.mlp.up_proj.weight False\n",
      "model.layers.20.mlp.down_proj.weight False\n",
      "model.layers.20.input_layernorm.weight True\n",
      "model.layers.20.post_attention_layernorm.weight True\n",
      "model.layers.20.pre_feedforward_layernorm.weight True\n",
      "model.layers.20.post_feedforward_layernorm.weight True\n",
      "model.layers.21.self_attn.q_proj.weight False\n",
      "model.layers.21.self_attn.k_proj.weight False\n",
      "model.layers.21.self_attn.v_proj.weight False\n",
      "model.layers.21.self_attn.o_proj.weight False\n",
      "model.layers.21.mlp.gate_proj.weight False\n",
      "model.layers.21.mlp.up_proj.weight False\n",
      "model.layers.21.mlp.down_proj.weight False\n",
      "model.layers.21.input_layernorm.weight True\n",
      "model.layers.21.post_attention_layernorm.weight True\n",
      "model.layers.21.pre_feedforward_layernorm.weight True\n",
      "model.layers.21.post_feedforward_layernorm.weight True\n",
      "model.layers.22.self_attn.q_proj.weight False\n",
      "model.layers.22.self_attn.k_proj.weight False\n",
      "model.layers.22.self_attn.v_proj.weight False\n",
      "model.layers.22.self_attn.o_proj.weight False\n",
      "model.layers.22.mlp.gate_proj.weight False\n",
      "model.layers.22.mlp.up_proj.weight False\n",
      "model.layers.22.mlp.down_proj.weight False\n",
      "model.layers.22.input_layernorm.weight True\n",
      "model.layers.22.post_attention_layernorm.weight True\n",
      "model.layers.22.pre_feedforward_layernorm.weight True\n",
      "model.layers.22.post_feedforward_layernorm.weight True\n",
      "model.layers.23.self_attn.q_proj.weight False\n",
      "model.layers.23.self_attn.k_proj.weight False\n",
      "model.layers.23.self_attn.v_proj.weight False\n",
      "model.layers.23.self_attn.o_proj.weight False\n",
      "model.layers.23.mlp.gate_proj.weight False\n",
      "model.layers.23.mlp.up_proj.weight False\n",
      "model.layers.23.mlp.down_proj.weight False\n",
      "model.layers.23.input_layernorm.weight True\n",
      "model.layers.23.post_attention_layernorm.weight True\n",
      "model.layers.23.pre_feedforward_layernorm.weight True\n",
      "model.layers.23.post_feedforward_layernorm.weight True\n",
      "model.layers.24.self_attn.q_proj.weight False\n",
      "model.layers.24.self_attn.k_proj.weight False\n",
      "model.layers.24.self_attn.v_proj.weight False\n",
      "model.layers.24.self_attn.o_proj.weight False\n",
      "model.layers.24.mlp.gate_proj.weight False\n",
      "model.layers.24.mlp.up_proj.weight False\n",
      "model.layers.24.mlp.down_proj.weight False\n",
      "model.layers.24.input_layernorm.weight True\n",
      "model.layers.24.post_attention_layernorm.weight True\n",
      "model.layers.24.pre_feedforward_layernorm.weight True\n",
      "model.layers.24.post_feedforward_layernorm.weight True\n",
      "model.layers.25.self_attn.q_proj.weight False\n",
      "model.layers.25.self_attn.k_proj.weight False\n",
      "model.layers.25.self_attn.v_proj.weight False\n",
      "model.layers.25.self_attn.o_proj.weight False\n",
      "model.layers.25.mlp.gate_proj.weight False\n",
      "model.layers.25.mlp.up_proj.weight False\n",
      "model.layers.25.mlp.down_proj.weight False\n",
      "model.layers.25.input_layernorm.weight True\n",
      "model.layers.25.post_attention_layernorm.weight True\n",
      "model.layers.25.pre_feedforward_layernorm.weight True\n",
      "model.layers.25.post_feedforward_layernorm.weight True\n",
      "model.layers.26.self_attn.q_proj.weight False\n",
      "model.layers.26.self_attn.k_proj.weight False\n",
      "model.layers.26.self_attn.v_proj.weight False\n",
      "model.layers.26.self_attn.o_proj.weight False\n",
      "model.layers.26.mlp.gate_proj.weight False\n",
      "model.layers.26.mlp.up_proj.weight False\n",
      "model.layers.26.mlp.down_proj.weight False\n",
      "model.layers.26.input_layernorm.weight True\n",
      "model.layers.26.post_attention_layernorm.weight True\n",
      "model.layers.26.pre_feedforward_layernorm.weight True\n",
      "model.layers.26.post_feedforward_layernorm.weight True\n",
      "model.layers.27.self_attn.q_proj.weight False\n",
      "model.layers.27.self_attn.k_proj.weight False\n",
      "model.layers.27.self_attn.v_proj.weight False\n",
      "model.layers.27.self_attn.o_proj.weight False\n",
      "model.layers.27.mlp.gate_proj.weight False\n",
      "model.layers.27.mlp.up_proj.weight False\n",
      "model.layers.27.mlp.down_proj.weight False\n",
      "model.layers.27.input_layernorm.weight True\n",
      "model.layers.27.post_attention_layernorm.weight True\n",
      "model.layers.27.pre_feedforward_layernorm.weight True\n",
      "model.layers.27.post_feedforward_layernorm.weight True\n",
      "model.layers.28.self_attn.q_proj.weight False\n",
      "model.layers.28.self_attn.k_proj.weight False\n",
      "model.layers.28.self_attn.v_proj.weight False\n",
      "model.layers.28.self_attn.o_proj.weight False\n",
      "model.layers.28.mlp.gate_proj.weight False\n",
      "model.layers.28.mlp.up_proj.weight False\n",
      "model.layers.28.mlp.down_proj.weight False\n",
      "model.layers.28.input_layernorm.weight True\n",
      "model.layers.28.post_attention_layernorm.weight True\n",
      "model.layers.28.pre_feedforward_layernorm.weight True\n",
      "model.layers.28.post_feedforward_layernorm.weight True\n",
      "model.layers.29.self_attn.q_proj.weight False\n",
      "model.layers.29.self_attn.k_proj.weight False\n",
      "model.layers.29.self_attn.v_proj.weight False\n",
      "model.layers.29.self_attn.o_proj.weight False\n",
      "model.layers.29.mlp.gate_proj.weight False\n",
      "model.layers.29.mlp.up_proj.weight False\n",
      "model.layers.29.mlp.down_proj.weight False\n",
      "model.layers.29.input_layernorm.weight True\n",
      "model.layers.29.post_attention_layernorm.weight True\n",
      "model.layers.29.pre_feedforward_layernorm.weight True\n",
      "model.layers.29.post_feedforward_layernorm.weight True\n",
      "model.layers.30.self_attn.q_proj.weight False\n",
      "model.layers.30.self_attn.k_proj.weight False\n",
      "model.layers.30.self_attn.v_proj.weight False\n",
      "model.layers.30.self_attn.o_proj.weight False\n",
      "model.layers.30.mlp.gate_proj.weight False\n",
      "model.layers.30.mlp.up_proj.weight False\n",
      "model.layers.30.mlp.down_proj.weight False\n",
      "model.layers.30.input_layernorm.weight True\n",
      "model.layers.30.post_attention_layernorm.weight True\n",
      "model.layers.30.pre_feedforward_layernorm.weight True\n",
      "model.layers.30.post_feedforward_layernorm.weight True\n",
      "model.layers.31.self_attn.q_proj.weight False\n",
      "model.layers.31.self_attn.k_proj.weight False\n",
      "model.layers.31.self_attn.v_proj.weight False\n",
      "model.layers.31.self_attn.o_proj.weight False\n",
      "model.layers.31.mlp.gate_proj.weight False\n",
      "model.layers.31.mlp.up_proj.weight False\n",
      "model.layers.31.mlp.down_proj.weight False\n",
      "model.layers.31.input_layernorm.weight True\n",
      "model.layers.31.post_attention_layernorm.weight True\n",
      "model.layers.31.pre_feedforward_layernorm.weight True\n",
      "model.layers.31.post_feedforward_layernorm.weight True\n",
      "model.layers.32.self_attn.q_proj.weight False\n",
      "model.layers.32.self_attn.k_proj.weight False\n",
      "model.layers.32.self_attn.v_proj.weight False\n",
      "model.layers.32.self_attn.o_proj.weight False\n",
      "model.layers.32.mlp.gate_proj.weight False\n",
      "model.layers.32.mlp.up_proj.weight False\n",
      "model.layers.32.mlp.down_proj.weight False\n",
      "model.layers.32.input_layernorm.weight True\n",
      "model.layers.32.post_attention_layernorm.weight True\n",
      "model.layers.32.pre_feedforward_layernorm.weight True\n",
      "model.layers.32.post_feedforward_layernorm.weight True\n",
      "model.layers.33.self_attn.q_proj.weight False\n",
      "model.layers.33.self_attn.k_proj.weight False\n",
      "model.layers.33.self_attn.v_proj.weight False\n",
      "model.layers.33.self_attn.o_proj.weight False\n",
      "model.layers.33.mlp.gate_proj.weight False\n",
      "model.layers.33.mlp.up_proj.weight False\n",
      "model.layers.33.mlp.down_proj.weight False\n",
      "model.layers.33.input_layernorm.weight True\n",
      "model.layers.33.post_attention_layernorm.weight True\n",
      "model.layers.33.pre_feedforward_layernorm.weight True\n",
      "model.layers.33.post_feedforward_layernorm.weight True\n",
      "model.layers.34.self_attn.q_proj.weight False\n",
      "model.layers.34.self_attn.k_proj.weight False\n",
      "model.layers.34.self_attn.v_proj.weight False\n",
      "model.layers.34.self_attn.o_proj.weight False\n",
      "model.layers.34.mlp.gate_proj.weight False\n",
      "model.layers.34.mlp.up_proj.weight False\n",
      "model.layers.34.mlp.down_proj.weight False\n",
      "model.layers.34.input_layernorm.weight True\n",
      "model.layers.34.post_attention_layernorm.weight True\n",
      "model.layers.34.pre_feedforward_layernorm.weight True\n",
      "model.layers.34.post_feedforward_layernorm.weight True\n",
      "model.layers.35.self_attn.q_proj.weight False\n",
      "model.layers.35.self_attn.k_proj.weight False\n",
      "model.layers.35.self_attn.v_proj.weight False\n",
      "model.layers.35.self_attn.o_proj.weight False\n",
      "model.layers.35.mlp.gate_proj.weight False\n",
      "model.layers.35.mlp.up_proj.weight False\n",
      "model.layers.35.mlp.down_proj.weight False\n",
      "model.layers.35.input_layernorm.weight True\n",
      "model.layers.35.post_attention_layernorm.weight True\n",
      "model.layers.35.pre_feedforward_layernorm.weight True\n",
      "model.layers.35.post_feedforward_layernorm.weight True\n",
      "model.layers.36.self_attn.q_proj.weight False\n",
      "model.layers.36.self_attn.k_proj.weight False\n",
      "model.layers.36.self_attn.v_proj.weight False\n",
      "model.layers.36.self_attn.o_proj.weight False\n",
      "model.layers.36.mlp.gate_proj.weight False\n",
      "model.layers.36.mlp.up_proj.weight False\n",
      "model.layers.36.mlp.down_proj.weight False\n",
      "model.layers.36.input_layernorm.weight True\n",
      "model.layers.36.post_attention_layernorm.weight True\n",
      "model.layers.36.pre_feedforward_layernorm.weight True\n",
      "model.layers.36.post_feedforward_layernorm.weight True\n",
      "model.layers.37.self_attn.q_proj.weight False\n",
      "model.layers.37.self_attn.k_proj.weight False\n",
      "model.layers.37.self_attn.v_proj.weight False\n",
      "model.layers.37.self_attn.o_proj.weight False\n",
      "model.layers.37.mlp.gate_proj.weight False\n",
      "model.layers.37.mlp.up_proj.weight False\n",
      "model.layers.37.mlp.down_proj.weight False\n",
      "model.layers.37.input_layernorm.weight True\n",
      "model.layers.37.post_attention_layernorm.weight True\n",
      "model.layers.37.pre_feedforward_layernorm.weight True\n",
      "model.layers.37.post_feedforward_layernorm.weight True\n",
      "model.layers.38.self_attn.q_proj.weight False\n",
      "model.layers.38.self_attn.k_proj.weight False\n",
      "model.layers.38.self_attn.v_proj.weight False\n",
      "model.layers.38.self_attn.o_proj.weight False\n",
      "model.layers.38.mlp.gate_proj.weight False\n",
      "model.layers.38.mlp.up_proj.weight False\n",
      "model.layers.38.mlp.down_proj.weight False\n",
      "model.layers.38.input_layernorm.weight True\n",
      "model.layers.38.post_attention_layernorm.weight True\n",
      "model.layers.38.pre_feedforward_layernorm.weight True\n",
      "model.layers.38.post_feedforward_layernorm.weight True\n",
      "model.layers.39.self_attn.q_proj.weight False\n",
      "model.layers.39.self_attn.k_proj.weight False\n",
      "model.layers.39.self_attn.v_proj.weight False\n",
      "model.layers.39.self_attn.o_proj.weight False\n",
      "model.layers.39.mlp.gate_proj.weight False\n",
      "model.layers.39.mlp.up_proj.weight False\n",
      "model.layers.39.mlp.down_proj.weight False\n",
      "model.layers.39.input_layernorm.weight True\n",
      "model.layers.39.post_attention_layernorm.weight True\n",
      "model.layers.39.pre_feedforward_layernorm.weight True\n",
      "model.layers.39.post_feedforward_layernorm.weight True\n",
      "model.layers.40.self_attn.q_proj.weight False\n",
      "model.layers.40.self_attn.k_proj.weight False\n",
      "model.layers.40.self_attn.v_proj.weight False\n",
      "model.layers.40.self_attn.o_proj.weight False\n",
      "model.layers.40.mlp.gate_proj.weight False\n",
      "model.layers.40.mlp.up_proj.weight False\n",
      "model.layers.40.mlp.down_proj.weight False\n",
      "model.layers.40.input_layernorm.weight True\n",
      "model.layers.40.post_attention_layernorm.weight True\n",
      "model.layers.40.pre_feedforward_layernorm.weight True\n",
      "model.layers.40.post_feedforward_layernorm.weight True\n",
      "model.layers.41.self_attn.q_proj.weight False\n",
      "model.layers.41.self_attn.k_proj.weight False\n",
      "model.layers.41.self_attn.v_proj.weight False\n",
      "model.layers.41.self_attn.o_proj.weight False\n",
      "model.layers.41.mlp.gate_proj.weight False\n",
      "model.layers.41.mlp.up_proj.weight False\n",
      "model.layers.41.mlp.down_proj.weight False\n",
      "model.layers.41.input_layernorm.weight True\n",
      "model.layers.41.post_attention_layernorm.weight True\n",
      "model.layers.41.pre_feedforward_layernorm.weight True\n",
      "model.layers.41.post_feedforward_layernorm.weight True\n",
      "model.norm.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Parammeters for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=CFG.LoRA_RANK,\n",
    "    lora_alpha=CFG.LoRA_ALPHA,\n",
    "    lora_dropout=CFG.LoRA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "    \"model.embed_tokens\", # able\n",
    "    #\"model.layers.0.input_layernorm\", # unable\n",
    "    #\"model.layers.0.post_attention_layernorm\", # unable\n",
    "    #\"model.layers.0.pre_feedforward_layernorm\", # unable\n",
    "    #\"model.layers.0.post_feedforward_layernorm\", # unable\n",
    "    #\"model.norm\" # unable\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")        \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CFG.TRAINING_RESUTL_DIR,\n",
    "    per_device_train_batch_size=CFG.PER_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=CFG.PER_EVAL_BATCH_SIZE,\n",
    "    num_train_epochs=CFG.NUM_TRAIN_EPOCHS,\n",
    "    logging_dir=CFG.LOGGING_DIR,\n",
    "    logging_steps=CFG.LOGGING_STEPS,\n",
    "    save_steps=CFG.SAVE_STEPS,\n",
    "    evaluation_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning and Saving Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Finetune_llm_with_SFT_Trainer():\n",
    "\n",
    "    def normalize_string(s):\n",
    "        \"\"\"유니코드 정규화\"\"\"\n",
    "        return unicodedata.normalize('NFC', s)\n",
    "    \n",
    "    # load dataset\n",
    "    # train_dataset = load_dataset('csv', data_files=CFG.EMBEDDING_CSV_TRAIN)['train']\n",
    "    # eval_dataset = load_dataset('csv', data_files=CFG.EMBEDDING_CSV_VAL)['train']  \n",
    "    train_dataset = load_dataset('csv', data_files=CFG.EMBEDDING_CSV_FULL_TRAIN)['train']\n",
    "    \n",
    "    def formatting_prompts_func(example):\n",
    "        output_texts = []\n",
    "        for i in range(len(example)):\n",
    "            text =  \"\"\"다음 정보를 바탕으로 질문에 답하세요. 답변은 꼭 문장으로 하세요. 주어를 꼭 적으세요. :\n",
    "# {example[Context]}\n",
    "# \n",
    "# 질문: {example[Question]}\n",
    "# \n",
    "# 답변: {example[Answer]}\n",
    "# \"\"\"\n",
    "            text = normalize_string(text)\n",
    "            output_texts.append(text)\n",
    "        return output_texts\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        peft_config = peft_config,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        train_dataset = train_dataset,\n",
    "        # eval_dataset = eval_dataset,   \n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save trained model\n",
    "    trainer.model.save_pretrained(CFG.SAVING_FINETUNING_MODEL_DIR)\n",
    "    tokenizer.save_pretrained(CFG.SAVING_FINETUNING_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 496 examples [00:00, 9350.04 examples/s]\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/496 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 496/496 [00:00<00:00, 24777.28 examples/s]\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtjwjddn15584\u001b[0m (\u001b[33mtjwjddn980117\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Seo\\Desktop\\gits\\2024_08_Search_Competition\\wandb\\run-20240808_174127-94vffpk7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tjwjddn980117/huggingface/runs/94vffpk7' target=\"_blank\">./finetune_models/training_result</a></strong> to <a href='https://wandb.ai/tjwjddn980117/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tjwjddn980117/huggingface' target=\"_blank\">https://wandb.ai/tjwjddn980117/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tjwjddn980117/huggingface/runs/94vffpk7' target=\"_blank\">https://wandb.ai/tjwjddn980117/huggingface/runs/94vffpk7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:458: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10.0235, 'train_samples_per_second': 1.496, 'train_steps_per_second': 0.998, 'train_loss': 2.4764156341552734, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "Finetune_llm_with_SFT_Trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Finetuned Model (for Quantization) and Make Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_SFTTrainer_with_finetuning():\n",
    "    # 4비트 양자화 설정\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # loading model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(CFG.SAVING_FINETUNING_MODEL_DIR, \n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 trust_remote_code=True,\n",
    "                                                 device_map=\"auto\",)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.SAVING_FINETUNING_MODEL_DIR)\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=CFG.PIPELINE_TEMPERATURE,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=CFG.SEQ_MAX_LENGHT, \n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = setup_llm_SFTTrainer_with_finetuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions:   0%|          | 0/98 [00:00<?, ?it/s]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:458: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Answering Questions:   1%|          | 1/98 [00:06<09:42,  6.00s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:   2%|▏         | 2/98 [00:19<16:11, 10.12s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:   3%|▎         | 3/98 [00:25<13:36,  8.59s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:   4%|▍         | 4/98 [00:29<10:33,  6.74s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:   5%|▌         | 5/98 [00:34<09:37,  6.21s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:   6%|▌         | 6/98 [00:38<08:10,  5.33s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:   7%|▋         | 7/98 [00:44<08:25,  5.55s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:   8%|▊         | 8/98 [00:48<07:46,  5.18s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:   9%|▉         | 9/98 [01:06<13:38,  9.20s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  10%|█         | 10/98 [01:14<12:38,  8.62s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  11%|█         | 11/98 [01:19<10:49,  7.47s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  12%|█▏        | 12/98 [01:22<09:06,  6.35s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  13%|█▎        | 13/98 [01:44<15:19, 10.82s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  14%|█▍        | 14/98 [01:46<11:47,  8.43s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  15%|█▌        | 15/98 [01:52<10:28,  7.57s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  16%|█▋        | 16/98 [02:05<12:27,  9.11s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  17%|█▋        | 17/98 [02:13<11:58,  8.87s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  18%|█▊        | 18/98 [02:20<10:54,  8.18s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  19%|█▉        | 19/98 [02:29<11:17,  8.57s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  20%|██        | 20/98 [02:33<09:08,  7.04s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  21%|██▏       | 21/98 [02:44<10:46,  8.40s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  22%|██▏       | 22/98 [02:48<09:01,  7.13s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  23%|██▎       | 23/98 [02:53<08:03,  6.45s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  24%|██▍       | 24/98 [03:00<08:16,  6.71s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  26%|██▌       | 25/98 [03:16<11:11,  9.20s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  27%|██▋       | 26/98 [03:19<09:01,  7.52s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  28%|██▊       | 27/98 [03:27<08:57,  7.57s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  29%|██▊       | 28/98 [03:32<07:57,  6.82s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  30%|██▉       | 29/98 [03:37<07:15,  6.31s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  31%|███       | 30/98 [03:42<06:32,  5.77s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  32%|███▏      | 31/98 [03:46<05:58,  5.34s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  33%|███▎      | 32/98 [03:51<05:46,  5.26s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  34%|███▎      | 33/98 [03:56<05:31,  5.10s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  35%|███▍      | 34/98 [04:15<09:59,  9.37s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  36%|███▌      | 35/98 [04:20<08:19,  7.92s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  37%|███▋      | 36/98 [04:28<08:14,  7.98s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  38%|███▊      | 37/98 [04:36<08:20,  8.20s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  39%|███▉      | 38/98 [04:47<09:04,  9.07s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  40%|███▉      | 39/98 [04:59<09:37,  9.79s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  41%|████      | 40/98 [05:06<08:36,  8.91s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  42%|████▏     | 41/98 [05:10<07:05,  7.46s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  43%|████▎     | 42/98 [05:22<08:15,  8.85s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  44%|████▍     | 43/98 [05:25<06:31,  7.12s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  45%|████▍     | 44/98 [05:31<06:11,  6.89s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  46%|████▌     | 45/98 [05:37<05:43,  6.49s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  47%|████▋     | 46/98 [05:46<06:17,  7.26s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  48%|████▊     | 47/98 [05:50<05:27,  6.42s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  49%|████▉     | 48/98 [06:03<06:56,  8.33s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  50%|█████     | 49/98 [06:22<09:24, 11.51s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  51%|█████     | 50/98 [06:37<10:05, 12.62s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  52%|█████▏    | 51/98 [06:45<08:37, 11.00s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  53%|█████▎    | 52/98 [06:54<07:59, 10.43s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  54%|█████▍    | 53/98 [07:02<07:27,  9.93s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  55%|█████▌    | 54/98 [07:11<06:56,  9.47s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  56%|█████▌    | 55/98 [07:29<08:42, 12.16s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  57%|█████▋    | 56/98 [07:39<08:04, 11.53s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  58%|█████▊    | 57/98 [07:47<06:59, 10.23s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  59%|█████▉    | 58/98 [07:52<05:53,  8.85s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  60%|██████    | 59/98 [08:02<05:50,  9.00s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  61%|██████    | 60/98 [08:12<05:58,  9.44s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  62%|██████▏   | 61/98 [08:17<04:56,  8.01s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  63%|██████▎   | 62/98 [08:21<04:13,  7.04s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  64%|██████▍   | 63/98 [08:25<03:26,  5.90s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  65%|██████▌   | 64/98 [08:33<03:40,  6.48s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  66%|██████▋   | 65/98 [08:39<03:35,  6.54s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  67%|██████▋   | 66/98 [08:53<04:41,  8.81s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  68%|██████▊   | 67/98 [09:02<04:27,  8.64s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  69%|██████▉   | 68/98 [09:12<04:38,  9.27s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  70%|███████   | 69/98 [09:23<04:40,  9.67s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  71%|███████▏  | 70/98 [09:30<04:05,  8.76s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  72%|███████▏  | 71/98 [09:48<05:11, 11.55s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  73%|███████▎  | 72/98 [09:58<04:53, 11.27s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  74%|███████▍  | 73/98 [10:05<04:09,  9.99s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  76%|███████▌  | 74/98 [10:24<04:59, 12.48s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  77%|███████▋  | 75/98 [10:47<06:06, 15.92s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  78%|███████▊  | 76/98 [11:01<05:31, 15.07s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  79%|███████▊  | 77/98 [11:11<04:45, 13.60s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  80%|███████▉  | 78/98 [11:16<03:44, 11.24s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  81%|████████  | 79/98 [11:21<02:54,  9.21s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  82%|████████▏ | 80/98 [11:27<02:27,  8.17s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  83%|████████▎ | 81/98 [11:32<02:04,  7.33s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  84%|████████▎ | 82/98 [11:41<02:03,  7.70s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  85%|████████▍ | 83/98 [11:47<01:48,  7.25s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  86%|████████▌ | 84/98 [11:53<01:34,  6.78s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  87%|████████▋ | 85/98 [12:02<01:39,  7.67s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  88%|████████▊ | 86/98 [12:09<01:27,  7.29s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  89%|████████▉ | 87/98 [12:14<01:12,  6.57s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  90%|████████▉ | 88/98 [12:22<01:09,  7.00s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  91%|█████████ | 89/98 [12:34<01:16,  8.55s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  92%|█████████▏| 90/98 [12:38<00:58,  7.27s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  93%|█████████▎| 91/98 [12:46<00:51,  7.38s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  94%|█████████▍| 92/98 [12:50<00:39,  6.54s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  95%|█████████▍| 93/98 [12:54<00:29,  5.86s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  96%|█████████▌| 94/98 [13:01<00:24,  6.11s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  97%|█████████▋| 95/98 [13:10<00:20,  6.87s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  98%|█████████▊| 96/98 [13:19<00:15,  7.55s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions:  99%|█████████▉| 97/98 [13:36<00:10, 10.40s/it]c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Answering Questions: 100%|██████████| 98/98 [13:51<00:00,  8.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(CFG.EMBEDDING_CSV_TEST)\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    context = normalize_string(row['Context'])\n",
    "    question = normalize_string(row['Question'])\n",
    "\n",
    "    # RAG 체인 구성\n",
    "    template = \"\"\"다음 정보를 바탕으로 질문에 답하세요. 답변은 꼭 문장으로 하세요. 주어를 꼭 적으세요. :\n",
    "    {context}\n",
    "\n",
    "    질문: {question}\n",
    "\n",
    "    답변:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # RAG 체인 정의\n",
    "    rag_chain = ( \n",
    "        prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 답변 추론\n",
    "    # print(f\"Question: {question}\")\n",
    "    full_response = rag_chain.invoke({\"context\":context, \"question\":question})\n",
    "\n",
    "    # print(f\"Answer: {full_response}\\n\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        'Question': question,\n",
    "        'Context': context,\n",
    "        'Answer': full_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSIOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(CFG.SAMPLE_SUBMISSION)\n",
    "\n",
    "# 생성된 답변을 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(CFG.RESULT_SUBMISSION, encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Search_Baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
