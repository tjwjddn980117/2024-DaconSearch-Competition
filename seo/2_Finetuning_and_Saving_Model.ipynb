{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Langchain 관련\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Root of Files\n",
    "    EMBEDDING_CSV_FULL_TRAIN = './datas/stf_e5_base_full_train.csv'\n",
    "    EMBEDDING_CSV_TRAIN = './datas/stf_e5_base_train.csv'\n",
    "    EMBEDDING_CSV_VAL = './datas/stf_e5_base_val.csv'\n",
    "\n",
    "    # About Finetuning\n",
    "    PRETRAINING_MODEL = \"rtzr/ko-gemma-2-9b-it\"\n",
    "\n",
    "    LoRA_RANK = 16\n",
    "    LoRA_ALPHA = 32\n",
    "    LoRA_DROPOUT = 0.05\n",
    "\n",
    "    TRAINING_RESUTL_DIR = './finetune_models/training_result'\n",
    "    PER_TRAIN_BATCH_SIZE =2\n",
    "    PER_EVAL_BATCH_SIZE = 2\n",
    "    NUM_TRAIN_EPOCHS = 5\n",
    "    LOGGING_DIR='./finetune_models/training_logs'\n",
    "    LOGGING_STEPS=1000\n",
    "    SAVE_STEPS=1000\n",
    "    SAVING_FINETUNING_MODEL_DIR = \"./finetune_models/gemma_ko_9b_ver1.01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading default Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 모델 ID \n",
    "model_id = CFG.PRETRAINING_MODEL\n",
    "\n",
    "# 토크나이저 로드 및 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.use_default_system_prompt = False\n",
    "\n",
    "# 모델 로드 및 양자화 설정 적용\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show how the model look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Parammeters for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=CFG.LoRA_RANK,\n",
    "    lora_alpha=CFG.LoRA_ALPHA,\n",
    "    lora_dropout=CFG.LoRA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "    \"model.embed_tokens\", # able\n",
    "    #\"model.layers.0.input_layernorm\", # unable\n",
    "    #\"model.layers.0.post_attention_layernorm\", # unable\n",
    "    #\"model.layers.0.pre_feedforward_layernorm\", # unable\n",
    "    #\"model.layers.0.post_feedforward_layernorm\", # unable\n",
    "    #\"model.norm\" # unable\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")        \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CFG.TRAINING_RESUTL_DIR,\n",
    "    per_device_train_batch_size=CFG.PER_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=CFG.PER_EVAL_BATCH_SIZE,\n",
    "    num_train_epochs=CFG.NUM_TRAIN_EPOCHS,\n",
    "    logging_dir=CFG.LOGGING_DIR,\n",
    "    logging_steps=CFG.LOGGING_STEPS,\n",
    "    save_steps=CFG.SAVE_STEPS,\n",
    "    evaluation_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning and Saving Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Finetune_llm_with_SFT_Trainer():\n",
    "\n",
    "    def normalize_string(s):\n",
    "        \"\"\"유니코드 정규화\"\"\"\n",
    "        return unicodedata.normalize('NFC', s)\n",
    "    \n",
    "    # load dataset\n",
    "    # train_dataset = load_dataset('csv', data_files=CFG.EMBEDDING_CSV_TRAIN)['train']\n",
    "    # eval_dataset = load_dataset('csv', data_files=CFG.EMBEDDING_CSV_VAL)['train']  \n",
    "    train_dataset = load_dataset('csv', data_files=CFG.EMBEDDING_CSV_FULL_TRAIN)['train']\n",
    "    \n",
    "    def formatting_prompts_func(example):\n",
    "        output_texts = []\n",
    "        for i in range(len(example)):\n",
    "            text =  \"\"\"다음 정보를 바탕으로 질문에 답하세요. 답변은 꼭 문장으로 하세요. 주어를 꼭 적으세요. :\n",
    "# {example[Context]}\n",
    "# \n",
    "# 질문: {example[Question]}\n",
    "# \n",
    "# 답변: {example[Answer]}\n",
    "# \"\"\"\n",
    "            text = normalize_string(text)\n",
    "            output_texts.append(text)\n",
    "        return output_texts\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        peft_config = peft_config,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        train_dataset = train_dataset,\n",
    "        # eval_dataset = eval_dataset,   \n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save trained model\n",
    "    trainer.model.save_pretrained(CFG.SAVING_FINETUNING_MODEL_DIR)\n",
    "    tokenizer.save_pretrained(CFG.SAVING_FINETUNING_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finetune_llm_with_SFT_Trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Search_Baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
