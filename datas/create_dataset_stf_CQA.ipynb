{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset for stf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "import pickle\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path, chunk_size=800, chunk_overlap=50):\n",
    "    \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n",
    "    # PDF 파일 열기\n",
    "    doc = fitz.open(file_path)\n",
    "    text = ''\n",
    "    # 모든 페이지의 텍스트 추출\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    # 텍스트를 chunk로 분할\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunk_temp = splitter.split_text(text)\n",
    "    # Document 객체 리스트 생성\n",
    "    chunks = [Document(page_content=t) for t in chunk_temp]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-base\"):\n",
    "    \"\"\"FAISS DB 생성\"\"\"\n",
    "    # 임베딩 모델 설정\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    # FAISS DB 생성 및 반환\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"경로 유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "\n",
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path)\n",
    "        db = create_vector_db(chunks)\n",
    "        \n",
    "        # Retriever 생성\n",
    "        retriever = db.as_retriever(search_type=\"mmr\", \n",
    "                                    search_kwargs={'k': 3, 'fetch_k': 8})\n",
    "        \n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "                'db': db,\n",
    "                'retriever': retriever\n",
    "        }\n",
    "    return pdf_databases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1-1 2024 주요 재정통계 1권...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "Processing PDFs:   6%|▋         | 1/16 [00:05<01:28,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024 나라살림 예산개요...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  12%|█▎        | 2/16 [00:12<01:26,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 재정통계해설...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  19%|█▉        | 3/16 [00:17<01:12,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 국토교통부_전세임대(융자)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▌       | 4/16 [00:20<00:58,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 고용노동부_청년일자리창출지원...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  31%|███▏      | 5/16 [00:24<00:48,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 고용노동부_내일배움카드(일반)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  38%|███▊      | 6/16 [00:28<00:42,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_노인일자리 및 사회활동지원...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|████▍     | 7/16 [00:32<00:37,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 중소벤처기업부_창업사업화지원...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 8/16 [00:35<00:31,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 보건복지부_생계급여...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|█████▋    | 9/16 [00:39<00:27,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 국토교통부_소규모주택정비사업...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  62%|██████▎   | 10/16 [00:42<00:22,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 국토교통부_민간임대(융자)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  69%|██████▉   | 11/16 [00:46<00:18,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 고용노동부_조기재취업수당...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|███████▌  | 12/16 [00:50<00:14,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024년도 성과계획서(총괄편)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  81%|████████▏ | 13/16 [01:35<00:48, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」 23-3호 《조세지출 연계관리》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  88%|████████▊ | 14/16 [01:42<00:26, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 「FIS 이슈 & 포커스」 22-3호 《재정융자사업》...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  94%|█████████▍| 15/16 [01:50<00:11, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 월간 나라재정 2023년 12월호...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 16/16 [02:52<00:00, 10.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_databases가 pickle 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "base_directory = \"./\"\n",
    "dataset_directory = base_directory + \"train.csv\"\n",
    "df = pd.read_csv(dataset_directory)\n",
    "pdf_databases = process_pdfs_from_dataframe(df, base_directory)\n",
    "\n",
    "# PDF 데이터베이스를 pickle 파일로 저장\n",
    "with open('pdf_databases_e5_base.pickle', 'wb') as f:\n",
    "    pickle.dump(pdf_databases, f)\n",
    "\n",
    "print(\"pdf_databases가 pickle 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Search_Baseline\\lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "base_directory = \"./\"\n",
    "dataset_directory = base_directory + \"train.csv\"\n",
    "df = pd.read_csv(dataset_directory)\n",
    "with open('pdf_databases_e5_base.pickle', 'rb') as f:\n",
    "    pdf_databases = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Q&A including RAG info: 100%|██████████| 496/496 [00:07<00:00, 65.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# 배치 사이즈 설정\n",
    "batch_size = 1  # 원하는 배치 크기로 설정\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for start in tqdm(range(0, len(df), batch_size), desc=\"Creating Q&A including RAG info\"):\n",
    "    # 현재 배치 선택\n",
    "    batch_rows = df.iloc[start:start + batch_size]\n",
    "\n",
    "    # 배치 내의 각 행 처리\n",
    "    for _, row in batch_rows.iterrows():\n",
    "        # 소스 문자열 정규화\n",
    "        source = normalize_string(row['Source'])\n",
    "        question = row['Question']\n",
    "        answer = row['Answer']\n",
    "\n",
    "        # 정규화된 키로 데이터베이스 검색\n",
    "        normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "        retriever = normalized_keys[source]['retriever']\n",
    "        \n",
    "        context = format_docs(retriever.invoke(question))\n",
    "\n",
    "        results.append({\n",
    "            \"Context\" : context,\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf_train_df = pd.DataFrame(results[:int(len(results)*0.8+0.5)])\n",
    "stf_eval_df = pd.DataFrame(results[int(len(results)*0.2+0.5):])\n",
    "\n",
    "stf_train_df.to_csv(\"stf_e5_base_train.csv\", index=False, encoding=\"UTF-8\")\n",
    "stf_eval_df.to_csv(\"stf_e5_base_eval.csv\", index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf_train = pd.read_csv(\"data/stf_train.csv\")\n",
    "print(stf_train.Question[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
